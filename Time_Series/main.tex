\documentclass[a4 paper]{article}
% Set target color model to RGB
\usepackage[inner=2.0cm,outer=2.0cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage{setspace}
\usepackage[rgb]{xcolor}
\usepackage{verbatim}
\usepackage{subcaption}
\usepackage{amsgen,amsmath,amstext,amsbsy,amsopn,tikz,amssymb,tkz-linknodes}
\usepackage{fancyhdr}
\usepackage[colorlinks=true, urlcolor=blue,  linkcolor=blue, citecolor=blue]{hyperref}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{rotating}
\usepackage[utf8]{inputenc} 
\usepackage[english]{babel}

%% Packages
\usepackage{scrextend}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage[inline]{enumitem}
\usepackage{changes}
\usepackage{chngcntr}
\usepackage{cmap}
\usepackage{color}
\usepackage{csquotes}
\usepackage{float}
\usepackage{hyperref}
\usepackage{footnote}
\usepackage{lmodern}
\usepackage{makeidx}
\usepackage{mathtools} 
\usepackage{xpatch}
\usepackage{pgfplots}
\usepackage{stmaryrd}
\usepackage{pbox}
\usepackage{apptools}
\usepackage{booktabs}
\usepackage{dsfont}
\usepackage{graphicx}
\usepackage{mathrsfs}
\usepackage[square,numbers]{natbib}
\usepackage{nicefrac}
\usepackage{pgf}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{tocloft}
\usepackage{url}
\usepackage{xpatch}
\usepackage{microtype}
\usepackage{pgfplots}
\usepackage{minibox}
\usepackage{xcolor}
\usepackage{sgame}  % Game theory packages


\makesavenoteenv{tabular}
\usepgfplotslibrary{fillbetween}
\usetikzlibrary{patterns}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{calc, intersections}
\usetikzlibrary{trees, calc} % For extensive form games
\pgfplotsset{compat=1.7}
\usetikzlibrary{calc}	
\usetikzlibrary{matrix}	

\usepgfplotslibrary{fillbetween}
\usetikzlibrary{patterns}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{calc, intersections}
\usetikzlibrary{trees, calc} % For extensive form games
\pgfplotsset{compat=1.7}
\usetikzlibrary{calc}	
\usetikzlibrary{matrix}	

%\usetikzlibrary{through,backgrounds}
\hypersetup{%
pdfauthor={MKL},%
pdftitle={Homework},%
pdfkeywords={Tikz,latex,bootstrap,uncertaintes},%
pdfcreator={PDFLaTeX},%
pdfproducer={PDFLaTeX},%
}
%\usetikzlibrary{shadows}
% \usepackage[francais]{babel}
\usepackage{booktabs}
\input{macros.tex}
\begin{document}

\homework{Time Series Analysis \#1}{Due: 23/06/19}{Many}{}{MKL(s)}{NetId(s)}
\textbf{Outline of the Module}: 
\begin{itemize}
    \item  AR, MA and ARMA models; ARCH and GARCH models 
    \item Option Pricing
    \item Hidden Markov Models, Kalman Filter
\end{itemize}

 


\pagenumbering{Alph}
\pagenumbering{arabic}
	
% Inhaltsverzeichnis
\tableofcontents
\thispagestyle{empty} 
  
% Skript - Anfang 


\section{Fundamentals needed} % (fold)
\label{sec:basics}

\paragraph{Concepts of convergence} % (fold)
\label{sub:convergence_in_distirbution}
There are three major concepts of convergence of random variables.
\begin{itemize}
    \item Convergence in distribution
    \item Convergence in probability
    \item Almost surely convergence
\end{itemize}

\begin{defn}[Convergence in distribution]
    $X_n \cid X$ if $\prob{X_n \leq x} \rightarrow \prob{ X \leq x}$ for all $x$.
\end{defn}

\begin{defn}[Convergence in probability]
    $X_n \cip X$ if $\prob{ (|X_n - X | \geq \epsilon)} \rightarrow 0$ for all $\epsilon > 0$.
\end{defn}

\begin{defn}[Almost surely convergence]
    $X_n \cas X$ if except on a null set $A$, $X_n \rightarrow X$, that is, $\lim_{n \rightarrow \infty} X_n = X$.  And hence $\prob{ \lim_{n \rightarrow \infty} X_n = X} = 1$
\end{defn}


\begin{defn}[$\sigma$-field generated by $X$]
    Let $X$ be a random variable defined on a probability space $(\Omega, \mathcal{F}, \P)$.  We call $\sigma(X)$ the $\sigma$-field generated by $X$, and we have \[
        \sigma(X) = \left\{ X^{-1}(B): B \in \mathcal{B} \right\}
    \] where $X^{-1}(B) = \{ \omega, X(\omega) \in \mathcal{B} \}$ and $\mathcal{B}$ is the Borel set on $\R$.
\end{defn}

\begin{defn}[Conditional probability]
    We have $\P( A | B) = \frac{\P(A, B)}{\P(B)}$ if $\P(B) \neq 0$.
\end{defn}

\begin{defn}[Naive conditional expectation]
    We have $\expc{X | B} = \frac{ \expc{ X \mathbb{I}_B}}{\P(B)}$.
\end{defn}

\begin{defn}[Conditional density]
    Let $g(x,y)$ be the joint density function for $X$ and $Y$.  Then we have $Y ~ \int_\R g(x,y)\, dx \equiv g_Y(y)$.  We also have \[
        g_{X|Y = y} = \frac{g(x,y)}{g_Y(y)}
    \] which defines the conditional density given $Y = y$.
    
    Finally, we define $\expc{ X |Y = y} = \int_\R x g_{X|Y = y}(x) \,dx$.
\end{defn}

\begin{defn}[Conditional expectation]
    Let $(\Omega, \mathcal{F}, \P)$ be a probability space.  Let $\mathcal{A}$ be a sub $\sigma$-field of $\mathcal{F}$.  Let $X$ be a random variable such that $\E(|X|) < \infty$.  We define $\expc{X | \mathcal{A}}$ to be a random variable $Z$ such that 
    \begin{enumerate}
        \item $Z$ is $\mathcal{A}$-measurable,
        \item $\E(X\mathbb{I}_A) = \E(Z\mathbb{I}_A)$ for all $A \in \mathcal{A}$.
    \end{enumerate}
\end{defn}


\begin{prop}[Properties of the conditional expectation]
    Consider $Z = \E(X|Y) = \E(X|\sigma(Y))$
    \begin{itemize}
        \item If $T$ is $\sigma(Y)$-measurable, then $\E(XT|Y) = T\E(X|Y)$ a.s.
        \item If $T$ is independent of $Y$, then $\E(T|Y) = \E(T)$.
        \item $\E(X) = \E(\E(X|T))$
    \end{itemize}
\end{prop}


\section{Stochastic Processes} % (fold)
A Stochastic Process is a collection of random variables ordered in time. 
\label{sec:stochastic_processes}
\begin{defn}[Markov process]
    A process $X = \{X_t : t \in T\}$ is a \textbf{Markov process} if 
    \[
        \prob{X(t) \leq x \, | \, X(t_1) = x_1, X(t_2) = x_2, \dots, X(t_n) = x_n} = \prob{X(t) \leq x \, | \, X(t_n) = x_n}
    \]
    whenever $t_1 < t_2 < \cdots < t_n < t$.

    In other words, a Markov process is a stochastic process satisfying the Markov property: the conditional distribution of future values is independent of the past.

\end{defn}


\begin{defn}[Markov chain]
    A \emph{Markov chain} is a stochastic process with a finite or countable state space is a Markov chain if it satisfies the Markov property.  Define the transition probabilities $P_{ij}$ by \[
        P_{ij} = \prob{X_{n+1} = j \, | \, X_n = i}
    \]
\end{defn}

\begin{defn}[Finite dimensional distribution]
    The \textbf{finite dimensional distribution} of a stochastic process $X$ is the joint distribution of $(X_{t_1}, X_{t_2}, \dots, X_{t_n})$
\end{defn}

\begin{defn}[Equality in distribution]
    Two random variables $X$ and $Y$ are \textbf{equal in distribution} if $\prob{X \leq \alpha} = \prob{Y \leq \alpha}$ for all $\alpha \in \R$.  We write $X \eqd Y$.
\end{defn}

\begin{defn}[Strictly stationary]
    A stochastic process $X$ is \textbf{strictly stationary} if \[
        (X_{t_1}, X_{t_2}, \dots, X_{t_n}) = (X_{t_1+h}, X_{t_2+h}, \dots, X_{t_n+h})
    \] for all $t_i, h$.
\end{defn}

\begin{defn}[Weakly stationary]
    A stochastic process $X$ is \textbf{weakly stationary} if $\expc{X_t} = \expc{X_{t+h}}$ and $\cov{X_t, X_s} = \cov{X_{t+h}, X_{s+h}}$ for all $t, s, h$.
\end{defn}

\begin{lem}
    If $\expc{X_t^2} < \infty$, then strictly stationary implies weakly stationary.
\end{lem}

\begin{example}{\ }
    \begin{itemize}
        \item The stochastic process $\{ X_t \}$ with $X_t$ all IID is strictly stationary.
        \item The stochastic process $W_t$ with $W_t ~ N(0,t)$ and $X_t - X_s$ independent of $X_s$ (for $s < t$) is not strictly or weakly stationary.
    \end{itemize}
\end{example}
\begin{defn}[Stationary increments]
    A stochastic process has \textbf{stationary increments} if 
    \[
        X_t - X_s \eqd X_{t-h} - X_{s-h}
    \] for all $s, t, h$.
\end{defn}

\begin{example}
    Let $X_n, n \geq 1$ be IID random variables.  Consider the stochastic process $\{ S_n \}$ where $S_n = \sum^n_{j=1} X_j$.  Then $\{ S_n \}$ has stationary increments.
\end{example}

\end{document} 
